{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aa57f2",
   "metadata": {},
   "source": [
    "# Assignment 20 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "##### 1. What is the underlying concept of Support Vector Machines ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73b7b03a",
   "metadata": {},
   "source": [
    "#### Ans-\n",
    "SVM or Support Vector Machine is a linear model for classification and regression problems. It can solve linear and non-linear problems and work well for many practical problems. The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "##### 2. What is the concept of a support vector ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b745a587",
   "metadata": {},
   "source": [
    "#### Ans-\n",
    "Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "##### 3. When using SVMs, why is it necessary to scale the inputs ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8db91c",
   "metadata": {},
   "source": [
    "#### Ans-\n",
    "Feature scaling is crucial for some machine learning algorithms, which consider distances between observations because the distance between two observations differs for non-scaled and scaled cases. As we've already stated, the decision boundary maximizes the distance to the nearest data points from different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "##### 4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4ae9b4",
   "metadata": {},
   "source": [
    "#### Ans-\n",
    "An SVM classifier can give the distance between the test instance and the decision boundary as output, so we can use that as a confidence score, but we cannot use this score to directly converted it into class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "##### 5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b695efe",
   "metadata": {},
   "source": [
    "#### Ans-\n",
    "This question applies only to linear SVMs since kernelized can only use the dual\n",
    "form. The computational complexity of the primal form of the SVM problem is proportional to the number of training instances m, while the computational complexity of the dual form is proportional to a number between m2 and m3. So if there are millions of instances, you should definitely use the primal form, because the dual form will be much too slow.\n",
    "The dual problem is faster to solve than the primal when the number of training\n",
    "instances is smaller than the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "##### 6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bae674f8",
   "metadata": {},
   "source": [
    "#### Ans-\n",
    "There might be too much regularization. To decrease it, you need to increase gamma or C (or both).\n",
    "Increasing gamma makes the bell-shape curve narrower, and as a result each instance's range of influence is smaller: the decision boundary ends up being more irregular, wiggling around individual instances. Conversely, a small gamma value makes the bell-shaped curve wider, so instances have a larger range of influence, and the decision boundary ends up smoother.\n",
    "A smaller C value leads to a wider street but more margin violations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b649e",
   "metadata": {},
   "source": [
    "##### 7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ac2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54020cf8",
   "metadata": {},
   "source": [
    "##### 8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f716511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9db681d1",
   "metadata": {},
   "source": [
    "##### 9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d926bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b686929",
   "metadata": {},
   "source": [
    "##### 10. On the California housing dataset, train an SVM regressor ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc128984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
